{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T12:52:18.104978Z",
     "start_time": "2020-12-13T12:52:18.102171Z"
    }
   },
   "source": [
    "# INITIALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T21:53:59.245244Z",
     "start_time": "2020-12-16T21:53:59.241573Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "%load_ext lab_black\n",
    "pd.set_option(\"display.max_rows\", 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T21:28:28.602199Z",
     "start_time": "2020-12-16T21:28:28.598168Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "IS_LOCAL = True\n",
    "SEED = 14\n",
    "GOOGLE_DRIVE_PATH = \"./drive/MyDrive/kaggle/Dota 2 prediction/\"\n",
    "\n",
    "PATH_TO_DATA = \"./data/\"\n",
    "PATH_TO_ADD_DATA = \"./add_data/\"\n",
    "if not IS_LOCAL:\n",
    "    PATH_TO_DATA = GOOGLE_DRIVE_PATH + PATH_TO_DATA\n",
    "    PATH_TO_ADD_DATA = GOOGLE_DRIVE_PATH + PATH_TO_ADD_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T09:34:57.126951Z",
     "start_time": "2020-12-15T09:34:57.054338Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ModuleNotFoundError:\n",
    "    import json\n",
    "\n",
    "    print(\"Please install ujson to read JSON oblects faster\")\n",
    "\n",
    "try:\n",
    "    from tqdm import notebook\n",
    "except ModuleNotFoundError:\n",
    "    tqdm_notebook = lambda x: x\n",
    "    print(\"Please install tqdm to track progress with Python loops\")\n",
    "\n",
    "\n",
    "def read_matches(matches_file):\n",
    "\n",
    "    MATCHES_COUNT = {\n",
    "        \"test_matches.jsonl\": 10000,\n",
    "        \"train_matches.jsonl\": 39675,\n",
    "    }\n",
    "    _, filename = os.path.split(matches_file)\n",
    "    total_matches = MATCHES_COUNT.get(filename)\n",
    "\n",
    "    with open(matches_file) as fin:\n",
    "        for line in notebook.tqdm(fin, total=total_matches):\n",
    "            yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T21:28:35.115543Z",
     "start_time": "2020-12-16T21:28:35.099617Z"
    },
    "code_folding": [
     5
    ],
    "hidden": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "ROLES = {\n",
    "    0: \"Durable\",\n",
    "    1: \"Carry\",\n",
    "    2: \"Disabler\",\n",
    "    3: \"Support\",\n",
    "    4: \"Pusher\",\n",
    "    5: \"Nuker\",\n",
    "    6: \"Escape\",\n",
    "    7: \"Initiator\",\n",
    "    8: \"Jungler\",\n",
    "}\n",
    "\n",
    "PRIMARY_ATTR = {\"agi\": 0, \"int\": 1, \"str\": 2}\n",
    "\n",
    "ATTACK_TYPE = {\"Melee\": 0, \"Ranged\": 1}\n",
    "\n",
    "\n",
    "def load_heroes():\n",
    "    i = 0\n",
    "    heroes = []\n",
    "    with open(PATH_TO_ADD_DATA + \"heroes.json\") as f:\n",
    "        heroes_data = json.load(f)\n",
    "\n",
    "    for k in heroes_data.keys():\n",
    "        hero = {}\n",
    "        hero[\"id\"] = heroes_data[k][\"id\"]\n",
    "        hero[\"name\"] = heroes_data[k][\"name\"][14:]\n",
    "\n",
    "        for attr in PRIMARY_ATTR:\n",
    "            hero[attr] = 1 if attr in heroes_data[k][\"primary_attr\"] else 0\n",
    "\n",
    "        for attack_type in ATTACK_TYPE:\n",
    "            hero[attack_type.lower()] = (\n",
    "                1 if attack_type in heroes_data[k][\"attack_type\"] else 0\n",
    "            )\n",
    "\n",
    "        for role in ROLES.values():\n",
    "            hero[role.lower()] = 1 if role in heroes_data[k][\"roles\"] else 0\n",
    "        heroes.append(hero)\n",
    "    return heroes\n",
    "\n",
    "\n",
    "heroes = load_heroes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:41.735609Z",
     "start_time": "2020-12-14T13:27:41.724550Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SIDES = [\"radiant\", \"dire\"]\n",
    "BARRACKS = {\n",
    "    \"radiant\": {\n",
    "        \"all\": [\"1\", \"2\", \"4\", \"8\", \"16\", \"32\"],\n",
    "        \"top\": [\"1\", \"2\"],\n",
    "        \"middle\": [\"4\", \"8\"],\n",
    "        \"bottom\": [\"16\", \"32\"],\n",
    "    },\n",
    "    \"dire\": {\n",
    "        \"all\": [\"64\", \"128\", \"256\", \"512\", \"1024\", \"2048\"],\n",
    "        \"top\": [\"64\", \"128\"],\n",
    "        \"middle\": [\"256\", \"512\"],\n",
    "        \"bottom\": [\"1024\", \"2048\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def process_objectives(objectives):\n",
    "    destroyed_barracks = {\"radiant\": [], \"dire\": []}\n",
    "    for objective in objectives:\n",
    "        if (\n",
    "            objective\n",
    "            and \"type\" in objective.keys()\n",
    "            and objective[\"type\"] == \"CHAT_MESSAGE_BARRACKS_KILL\"\n",
    "        ):\n",
    "            if objective[\"key\"] in BARRACKS[\"radiant\"][\"all\"]:\n",
    "                destroyed_barracks[\"dire\"] += [objective[\"key\"]]\n",
    "            elif objective[\"key\"] in BARRACKS[\"dire\"][\"all\"]:\n",
    "                destroyed_barracks[\"radiant\"] += [objective[\"key\"]]\n",
    "            else:\n",
    "                print('Error: undefined key \"%s\"in counting of barracks' % s[\"key\"])\n",
    "    extended_destroyed_barracks = {\n",
    "        \"r_destroyed_barracks\": len(destroyed_barracks[\"radiant\"]),\n",
    "        \"d_destroyed_barracks\": len(destroyed_barracks[\"dire\"]),\n",
    "    }\n",
    "    reversed_sides = [\"dire\", \"radiant\"]\n",
    "    for side in [\"radiant\", \"dire\"]:\n",
    "        opposite_side = reversed_sides.pop(0)\n",
    "        side_dict = {}\n",
    "        for position in [\"top\", \"middle\", \"bottom\"]:\n",
    "            side_dict[f\"{side[0]}_destroyed_{position}\"] = (\n",
    "                np.all(\n",
    "                    [\n",
    "                        True if barrack in destroyed_barracks[side] else False\n",
    "                        for barrack in BARRACKS[opposite_side][position]\n",
    "                    ]\n",
    "                )\n",
    "            ) * 1\n",
    "        side_dict[f\"{side[0]}_supercreeps\"] = np.all(list(side_dict.values())) * 1\n",
    "        extended_destroyed_barracks.update(side_dict)\n",
    "    return extended_destroyed_barracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:27:42.068206Z",
     "start_time": "2020-12-14T13:27:42.058962Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_teamfights(teamfights):\n",
    "    feats_to_process = [\"gold_delta\", \"xp_delta\", \"damage\", \"buybacks\"]\n",
    "    to_row = {}\n",
    "    if teamfights:\n",
    "        td = {\n",
    "            col: np.sum(\n",
    "                [[pl[col] for pl in teamfight[\"players\"]] for teamfight in teamfights],\n",
    "                axis=0,\n",
    "            )\n",
    "            for col in feats_to_process\n",
    "        }\n",
    "        teamfights_dict = {}\n",
    "        for side, idxs in zip(SIDES, [range(5), range(5, 10)]):\n",
    "            teamfights_dict.update({side: {}})\n",
    "            for col in feats_to_process:\n",
    "                teamfights_dict[side].update({col: {}})\n",
    "                teamfights_dict[side][col].update(\n",
    "                    {f\"{side[0]}{idx%5+1}_{col}\": td[col][idx] for idx in idxs}\n",
    "                )\n",
    "                teamfights_dict[side][col][f\"{side[0]}_pt_{col}\"] = sum(\n",
    "                    teamfights_dict[side][col].values()\n",
    "                )\n",
    "                to_row.update(teamfights_dict[side][col])\n",
    "        to_row[\"was_teamfight\"] = True * 1\n",
    "    else:\n",
    "        for side in SIDES:\n",
    "            for col in feats_to_process:\n",
    "                for n in range(5):\n",
    "                    to_row[f\"{side[0]}{n+1}_{col}\"] = 0\n",
    "                    to_row[f\"{side[0]}_pt_{col}\"] = 0\n",
    "            to_row[f\"{side[0]}_pt_{col}\"] = 0\n",
    "        to_row[\"was_teamfight\"] = False * 1\n",
    "    return to_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:49:44.155735Z",
     "start_time": "2020-12-14T13:49:44.148008Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PLAYER_COLS = [\n",
    "    \"pred_vict\",\n",
    "    \"gold\",\n",
    "    \"lh\",\n",
    "    \"xp\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"max_health\",\n",
    "    \"max_mana\",\n",
    "    \"kills\",\n",
    "    \"deaths\",\n",
    "    \"assists\",\n",
    "    \"denies\",\n",
    "    \"nearby_creep_death_count\",\n",
    "    \"roshans_killed\",\n",
    "    \"hero_id\",\n",
    "    \"account_id_hash\",\n",
    "    \"obs_placed\",\n",
    "    \"sen_placed\",\n",
    "    \"creeps_stacked\",\n",
    "    \"camps_stacked\",\n",
    "    \"rune_pickups\",\n",
    "    \"firstblood_claimed\",\n",
    "    \"teamfight_participation\",\n",
    "    \"towers_killed\",\n",
    "    \"stuns\",\n",
    "]\n",
    "TIMES_COLS = [\n",
    "    \"times\",\n",
    "    \"gold_t\",\n",
    "    \"lh_t\",\n",
    "    \"dn_t\",\n",
    "    \"xp_t\",\n",
    "    \"obs_log\",\n",
    "    \"sen_log\",\n",
    "    \"obs_left_log\",\n",
    "    \"sen_left_log\",\n",
    "    \"buyback_log\",\n",
    "]\n",
    "COUNT_ = [\n",
    "    \"purchase\",\n",
    "    \"actions\",\n",
    "    \"gold_reasons\",\n",
    "    \"xp_reasons\",\n",
    "    \"item_uses\",\n",
    "    \"hero_hits\",\n",
    "    \"kill_streaks\",\n",
    "    \"multi_kills\",\n",
    "    \"ability_uses\",\n",
    "    \"healing\",\n",
    "]\n",
    "\n",
    "POWERFUL_ITEMS = [\"rapier\", \"aegis\", \"moon_shard\", \"gem\", \"cheese\"]\n",
    "\n",
    "TIME_COLS = [\"gold_t\", \"lh_t\", \"xp_t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:40:36.678836Z",
     "start_time": "2020-12-14T13:40:36.670307Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_players(players):\n",
    "    players_info = {}\n",
    "    for player_num in range(len(players)):\n",
    "        if player_num < 5:\n",
    "            side = \"r\"  # radiant\n",
    "        else:\n",
    "            side = \"d\"  # dire\n",
    "        temp_player = players[player_num]\n",
    "        prefix = (\n",
    "            side + str(player_num + 1) + \"_\"\n",
    "            if side == \"r\"\n",
    "            else side + str(player_num - 4) + \"_\"\n",
    "        )\n",
    "        for col in PLAYER_COLS:\n",
    "            players_info[prefix + col] = temp_player[col]\n",
    "        for col in COUNT_:\n",
    "            players_info[prefix + col] = sum(temp_player[col].values())\n",
    "        player_items = [el[\"id\"][5:] for el in temp_player[\"hero_inventory\"]]\n",
    "        for col in TIME_COLS:\n",
    "            if len(temp_player[col]) > 1:\n",
    "                players_info[prefix + col] = temp_player[col][-1] - temp_player[col][-2]\n",
    "            else:\n",
    "                players_info[prefix + col] = 0\n",
    "        for item in POWERFUL_ITEMS:\n",
    "            players_info[prefix + \"item_\" + item] = 1 if item in player_items else 0\n",
    "\n",
    "    return players_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:40:36.896667Z",
     "start_time": "2020-12-14T13:40:36.887703Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data_jsonl(name, start=0, end=-1, save=False, infunc=None):\n",
    "    i = 0\n",
    "    rows = []\n",
    "    for match in read_matches(os.path.join(PATH_TO_DATA, \"%s_matches.jsonl\" % name)):\n",
    "        while i <= start:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        row = {}\n",
    "\n",
    "        # GENERAL INFO ABOUT MATCH\n",
    "        row[\"match_id_hash\"] = match[\"match_id_hash\"]\n",
    "        row[\"game_time\"] = match[\"game_time\"]\n",
    "        row[\"game_mode_id\"] = match[\"game_mode\"]\n",
    "        row[\"lobby_type_id\"] = match[\"lobby_type\"]\n",
    "\n",
    "        # OBJECTIVES\n",
    "        row.update(process_objectives(match[\"objectives\"]))\n",
    "\n",
    "        # TEAMFIGHTS\n",
    "        row.update(process_teamfights(match[\"teamfights\"]))\n",
    "\n",
    "        # PLAYERS\n",
    "        row.update(process_players(match[\"players\"]))\n",
    "\n",
    "        # TARGETS\n",
    "        if name == \"train\":\n",
    "            for k in match[\"targets\"].keys():\n",
    "                row[\"target_\" + k] = match[\"targets\"][k]\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        if i == end:\n",
    "            break\n",
    "        i += 1\n",
    "    df = pd.DataFrame(rows)\n",
    "    try:\n",
    "        if save:\n",
    "            df.to_csv(f\"./data/{name}_hands_features.csv\", index=\"match_id_hash\")\n",
    "    except:\n",
    "        print(\"Error: saving failed!\")\n",
    "    return df, match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:43:14.936718Z",
     "start_time": "2020-12-14T13:40:38.991936Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_ = load_data_jsonl('test', save=True)\n",
    "train_ = load_data_jsonl('train', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T13:57:47.498858Z",
     "start_time": "2020-12-09T13:57:47.492656Z"
    }
   },
   "source": [
    "# CREATE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T21:28:39.838658Z",
     "start_time": "2020-12-16T21:28:39.831354Z"
    },
    "code_folding": [
     0
    ],
    "tags": [
     "func"
    ]
   },
   "outputs": [],
   "source": [
    "def intel_delete_column_rd(df, cols, output=False, get_pcols=False):\n",
    "    if type(cols) is str:\n",
    "        cols = [cols]\n",
    "\n",
    "    pcols = []\n",
    "    for col in cols:\n",
    "        if col.startswith(\"FEP_\"):\n",
    "            pcols += [f\"{s}{n}_{col[4:]}\" for n in list(\"12345\") for s in [\"r\", \"d\"]]\n",
    "        elif col.startswith(\"FET_\"):\n",
    "            pcols += [f\"{s}_{col[4:]}\" for s in [\"r\", \"d\"]]\n",
    "        elif col.startswith(\"FEM_\"):\n",
    "            pcols.append(col[4:])\n",
    "    if get_pcols:\n",
    "        return pcols\n",
    "    fcols = []\n",
    "    for col in pcols:\n",
    "        if col in df.columns:\n",
    "            fcols.append(col)\n",
    "        elif output:\n",
    "            print('there is no \"%s\" column!' % col)\n",
    "\n",
    "    return df.drop(fcols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:01.870016Z",
     "start_time": "2020-12-16T22:02:58.895089Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"train_hands_features.csv\"), index_col=\"match_id_hash\"\n",
    ").drop(\"Unnamed: 0\", axis=1)\n",
    "df_test = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"test_hands_features.csv\"), index_col=\"match_id_hash\"\n",
    ").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:01.882040Z",
     "start_time": "2020-12-16T22:03:01.873127Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "info_cols = [col for idx, col in enumerate(df_train.columns) if col[1].isalpha()]\n",
    "match_info = [col for col in info_cols if col[:6] != \"target\"]\n",
    "target_info = [col for col in info_cols if col[:6] == \"target\"]\n",
    "\n",
    "players_info = sorted(list(set(df_train.columns) - set(info_cols))[::-1])\n",
    "radiant_info = [col for col in players_info if col[0] == \"r\" and col[1].isnumeric()]\n",
    "dire_info = [col for col in players_info if col[0] == \"d\" and col[1].isnumeric()]\n",
    "\n",
    "radiant_team_info = [col for col in players_info if col.startswith(\"r_\")]\n",
    "dire_team_info = [col for col in players_info if col.startswith(\"d_\")]\n",
    "\n",
    "other_info = set(df_train.columns) - set(\n",
    "    info_cols\n",
    "    + match_info\n",
    "    + target_info\n",
    "    + radiant_info\n",
    "    + dire_info\n",
    "    + radiant_team_info\n",
    "    + dire_team_info\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:02.190716Z",
     "start_time": "2020-12-16T22:03:01.885499Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "uv = df_train.copy()\n",
    "uv = df_train[\n",
    "    pd.Index(\n",
    "        match_info\n",
    "        + dire_info\n",
    "        + dire_team_info\n",
    "        + radiant_info\n",
    "        + radiant_team_info\n",
    "        + target_info\n",
    "    )\n",
    "]\n",
    "uv.columns = pd.Index(\n",
    "    match_info\n",
    "    + radiant_info\n",
    "    + radiant_team_info\n",
    "    + dire_info\n",
    "    + dire_team_info\n",
    "    + target_info\n",
    ")\n",
    "xs_ys = [\n",
    "    col for col in uv.columns if (col[-1] == \"x\" or col[-1] == \"y\") and len(col) <= 4\n",
    "]\n",
    "uv.loc[:, xs_ys] = 256 - uv.loc[:, xs_ys]\n",
    "uv[\"target_radiant_win\"] = ~uv[\"target_radiant_win\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:02.219351Z",
     "start_time": "2020-12-16T22:03:02.194173Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "uv.index = pd.Index([\"mirror_\" + id_[7:] for id_ in uv.index])\n",
    "\n",
    "uv[\"is_mirror\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:03.357571Z",
     "start_time": "2020-12-16T22:03:02.222227Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "tdf = df_train.append(df_test)\n",
    "tdf[\"is_mirror\"] = 0\n",
    "\n",
    "df = tdf.append(uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:04.198220Z",
     "start_time": "2020-12-16T22:03:03.360405Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"target_radiant_win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:04.453081Z",
     "start_time": "2020-12-16T22:03:04.202998Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "df.insert(2, \"is_ranked_lobby\", (df[\"lobby_type_id\"] == 7).astype(np.uint8))\n",
    "df.insert(2, \"is_public_lobby\", (df[\"lobby_type_id\"] == 0).astype(np.uint8))\n",
    "df.drop(\"lobby_type_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:04.465759Z",
     "start_time": "2020-12-16T22:03:04.457164Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feats(\n",
    "    cols,\n",
    "    prefixes=\"\",\n",
    "    seps=\"_\",\n",
    "    suffixes=\"\",\n",
    "    feats_type=\"fep\",\n",
    "    sides=[\"r\", \"d\"],\n",
    "    together=True,\n",
    "):\n",
    "    # For Each Player => fep\n",
    "    # For Each Team => fet\n",
    "    tolist = lambda x: [x] if type(x) is not list else x\n",
    "    cols, prefixes, seps, suffixes, sides = map(\n",
    "        tolist, [cols, prefixes, seps, suffixes, sides]\n",
    "    )\n",
    "    cols = tolist(cols)\n",
    "    seps = tolist(seps)\n",
    "\n",
    "    players_num = range(1, 6) if feats_type == \"fep\" else [\"\"]\n",
    "\n",
    "    prepared_feats = np.array(\n",
    "        [\n",
    "            [\n",
    "                f\"{prefix}{side}{player_num}{sep}{col}{suffix}\"\n",
    "                for col in cols\n",
    "                for prefix in prefixes\n",
    "                for suffix in suffixes\n",
    "                for sep in seps\n",
    "                for player_num in players_num\n",
    "            ]\n",
    "            for side in sides\n",
    "        ]\n",
    "    )\n",
    "    if together:\n",
    "        return prepared_feats.flatten()\n",
    "    else:\n",
    "        return prepared_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:04.721844Z",
     "start_time": "2020-12-16T22:03:04.469561Z"
    },
    "code_folding": [],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.insert(1, \"is_turbo_mode\", (df[\"game_mode_id\"] == 23).astype(np.uint8).values)\n",
    "df.drop(\"game_mode_id\", axis=1, inplace=True)\n",
    "\n",
    "df[get_feats(\"pred_vict\")] = df[get_feats(\"pred_vict\")].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:05.838990Z",
     "start_time": "2020-12-16T22:03:04.724003Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "pheroes = pd.DataFrame(heroes)\n",
    "pheroes.set_index(\"id\", inplace=True)\n",
    "for s in [\"r\", \"d\"]:\n",
    "    for n in range(1, 6):\n",
    "        prefix = f\"{s}{n}_\"\n",
    "        df_heroes_info = (\n",
    "            df[[f\"{prefix}hero_id\"]]\n",
    "            .merge(pheroes, left_on=f\"{prefix}hero_id\", right_on=\"id\", how=\"left\")\n",
    "            .iloc[:, 1:]\n",
    "            .add_prefix(f\"{prefix}hero_\")\n",
    "        )\n",
    "        for col in df_heroes_info.columns:\n",
    "            df.insert(0, col, df_heroes_info.loc[:, col].values)\n",
    "            if df[col].nunique() == 2:\n",
    "                df[col] = df[col].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:06.394238Z",
     "start_time": "2020-12-16T22:03:05.841116Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# Create features xy if x & y are similar and large near dire throne, if similar and low near radiant throne\n",
    "df[get_feats(\"xy\")] = df[get_feats(\"x\")].values + df[get_feats(\"y\")].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:07.994400Z",
     "start_time": "2020-12-16T22:03:06.397068Z"
    }
   },
   "outputs": [],
   "source": [
    "# in account_id_hash we see very often the same account_id_hash, maybe it is valuable for us\n",
    "most_frequent_acc_id_hash = (\n",
    "    df[get_feats(\"account_id_hash\")].mode().mode(axis=1)[0].values[0]\n",
    ")\n",
    "\n",
    "df[get_feats(\"is_mf_hash\")] = (\n",
    "    df[get_feats(\"account_id_hash\")] == most_frequent_acc_id_hash\n",
    ") * 1\n",
    "df.drop(get_feats(\"account_id_hash\"), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:03:08.221964Z",
     "start_time": "2020-12-16T22:03:07.996782Z"
    }
   },
   "outputs": [],
   "source": [
    "# kda of every hero\n",
    "df[get_feats(\"kda\")] = (\n",
    "    df[get_feats(\"kills\")].values + df[get_feats(\"assists\")].values\n",
    ") / (df[get_feats(\"deaths\")].values + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:09:54.408674Z",
     "start_time": "2020-12-16T22:09:53.394119Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feats gold per minute (gold_pm), xp_pm, kda_pm\n",
    "df[get_feats([\"gold\", \"xp\", \"kda\"], suffixes=\"_pm\")] = df[\n",
    "    get_feats([\"gold\", \"xp\", \"kda\"])\n",
    "].values / (np.tile(df[\"game_time\"].replace({0: 99999}).values, (30, 1)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:04:08.010958Z",
     "start_time": "2020-12-16T22:03:29.264753Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get info about feats of evry player by sum, mean, max, min\n",
    "# ex.: 'r1_gold', 'r2_gold', 'r3_gold', 'r4_gold', 'r5_gold' -> 'r_pt_gold', 'r_mean_gold', 'r_max_gold', 'r_min_gold'\n",
    "feats_to_group = [\n",
    "    \"hero_agi\",\n",
    "    \"hero_disabler\",\n",
    "    \"firstblood_claimed\",\n",
    "    \"healing\",\n",
    "    \"xy\",\n",
    "    \"hero_nuker\",\n",
    "    \"gold\",\n",
    "    \"lh\",\n",
    "    \"hero_jungler\",\n",
    "    \"hero_str\",\n",
    "    \"hero_carry\",\n",
    "    \"rune_pickups\",\n",
    "    \"hero_ranged\",\n",
    "    \"roshans_killed\",\n",
    "    \"hero_escape\",\n",
    "    \"kda\",\n",
    "    \"obs_placed\",\n",
    "    \"xp\",\n",
    "    \"hero_pusher\",\n",
    "    \"damage\",\n",
    "    \"towers_killed\",\n",
    "    \"is_mf_hash\",\n",
    "    \"denies\",\n",
    "    \"hero_initiator\",\n",
    "    \"assists\",\n",
    "    \"hero_support\",\n",
    "    \"hero_melee\",\n",
    "    \"kills\",\n",
    "    \"hero_durable\",\n",
    "    \"deaths\",\n",
    "    \"item_aegis\",\n",
    "    \"hero_hits\",\n",
    "    \"multi_kills\",\n",
    "    \"hero_int\",\n",
    "    \"sen_placed\",\n",
    "    \"pred_vict\",\n",
    "    \"camps_stacked\",\n",
    "    \"creeps_stacked\",\n",
    "    \"max_health\",\n",
    "    \"max_mana\",\n",
    "    \"stuns\",\n",
    "    \"teamfight_participation\",\n",
    "    \"gold_t\",\n",
    "    \"lh_t\",\n",
    "    \"xp_t\",\n",
    "]\n",
    "\n",
    "# Choose way\n",
    "GROUPED_FEATS = {\n",
    "    \"Fastest but hard explainable\": 1,\n",
    "    \"Middle in speed and explainability\": 2,\n",
    "    \"Slowest but clear\": 3,\n",
    "}\n",
    "THIS_TIME_WAY = GROUPED_FEATS[\"Fastest but hard explainable\"]\n",
    "\n",
    "if THIS_TIME_WAY < 3:\n",
    "    sum_mean_max_min = lambda feat: np.array(\n",
    "        [feat.sum(axis=1), feat.mean(axis=1), feat.max(axis=1), feat.min(axis=1)]\n",
    "    ).T\n",
    "\n",
    "    GROUPS = [\"_pt_\", \"_mean_\", \"_max_\", \"_min_\"]\n",
    "    N_GROUPS = len(GROUPS)\n",
    "    N_PLAYERS = 5\n",
    "\n",
    "if THIS_TIME_WAY == GROUPED_FEATS['Fastest but hard explainable']:\n",
    "    feats_to_group_fet_radiant, feats_to_group_fet_dire = get_feats(\n",
    "        feats_to_group, seps=GROUPS, feats_type=\"fet\", together=False\n",
    "    )\n",
    "    feats_to_group_fep_radiant, feats_to_group_fep_dire = get_feats(\n",
    "        feats_to_group, together=False\n",
    "    )\n",
    "    for feat_id, feat_name in enumerate(feats_to_group):\n",
    "        df.loc[\n",
    "            :,\n",
    "            feats_to_group_fet_radiant[feat_id * N_GROUPS : feat_id * N_GROUPS + N_GROUPS],\n",
    "        ] = sum_mean_max_min(\n",
    "            df.loc[\n",
    "                :,\n",
    "                feats_to_group_fep_radiant[\n",
    "                    feat_id * N_PLAYERS : feat_id * N_PLAYERS + N_PLAYERS\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "        df.loc[\n",
    "            :, feats_to_group_fet_dire[feat_id * N_GROUPS : feat_id * N_GROUPS + N_GROUPS]\n",
    "        ] = sum_mean_max_min(\n",
    "            df.loc[\n",
    "                :,\n",
    "                feats_to_group_fep_dire[\n",
    "                    feat_id * N_PLAYERS : feat_id * N_PLAYERS + N_PLAYERS\n",
    "                ],\n",
    "            ]\n",
    "        )\n",
    "elif THIS_TIME_WAY == GROUPED_FEATS['Middle in speed and explainability']:\n",
    "    for feat in feats_to_group:\n",
    "\n",
    "        fet_radiant_feats, fet_dire_feats = get_feats(\n",
    "            feat, seps=GROUPS, feats_type=\"fet\", together=False\n",
    "        )\n",
    "        fep_radiant_feats, fep_dire_feats = get_feats(feat, together=False)\n",
    "\n",
    "        df.loc[:, fet_radiant_feats] = sum_mean_max_min(df.loc[:, fep_radiant_feats])\n",
    "        df.loc[:, fet_dire_feats] = sum_mean_max_min(df.loc[:, fep_dire_feats])\n",
    "\n",
    "elif THIS_TIME_WAY == GROUPED_FEATS[\"Slowest but clear\"]:\n",
    "    for col in feats_to_group:\n",
    "        radiant_summary = [f\"r{n}_{col}\" for n in range(1, 6)]\n",
    "        dire_summary = [f\"d{n}_{col}\" for n in range(1, 6)]\n",
    "\n",
    "        df[\"r_pt_\" + col] = df[radiant_summary].sum(axis=1)\n",
    "        df[\"d_pt_\" + col] = df[dire_summary].sum(axis=1)\n",
    "\n",
    "        df[\"r_mean_\" + col] = df[radiant_summary].mean(axis=1)\n",
    "        df[\"d_mean_\" + col] = df[dire_summary].mean(axis=1)\n",
    "\n",
    "        df[\"r_max_\" + col] = df[radiant_summary].max(axis=1)\n",
    "        df[\"d_max_\" + col] = df[dire_summary].max(axis=1)\n",
    "\n",
    "        df[\"r_min_\" + col] = df[radiant_summary].min(axis=1)\n",
    "        df[\"d_min_\" + col] = df[dire_summary].min(axis=1)\n",
    "else:\n",
    "    print(\"Choose wisely your dear, stalker\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:06:27.297688Z",
     "start_time": "2020-12-16T22:04:08.013520Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "map_center = (127.0, 127.0)\n",
    "\n",
    "\n",
    "def rotate(origin, point, angle):\n",
    "    # Rotate a point counterclockwise by a given angle around a given origin.\n",
    "    # The angle should be given in radians.\n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "\n",
    "    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n",
    "    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n",
    "    return qx, qy\n",
    "\n",
    "\n",
    "for col in [\n",
    "    f\"{s}{n}_{col}_rot\" for col in [\"x\", \"y\"] for s in [\"r\", \"d\"] for n in range(1, 6)\n",
    "]:\n",
    "    df[col] = df.apply(\n",
    "        lambda row: rotate(\n",
    "            map_center, (row[f\"{col[:2]}_x\"], row[f\"{col[:2]}_y\"]), 0.785398\n",
    "        )[0],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:06:27.312545Z",
     "start_time": "2020-12-16T22:06:27.301191Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    ShuffleSplit,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "plt.rc(\"axes\", titlesize=18)\n",
    "plt.rc(\"axes\", labelsize=18)\n",
    "plt.rc(\"xtick\", labelsize=16)\n",
    "plt.rc(\"ytick\", labelsize=16)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:07:00.403526Z",
     "start_time": "2020-12-16T22:07:00.395113Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_items():\n",
    "\n",
    "    train_df = pd.read_csv(\n",
    "        os.path.join(PATH_TO_ADD_DATA, \"train_items.csv\"), index_col=\"match_id_hash\"\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(PATH_TO_ADD_DATA, \"test_items.csv\"), index_col=\"match_id_hash\"\n",
    "    )\n",
    "    train_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    test_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    y_train = (\n",
    "        pd.read_csv(PATH_TO_DATA + \"train_targets.csv\", index_col=\"match_id_hash\")[\n",
    "            \"radiant_win\"\n",
    "        ]\n",
    "        * 1\n",
    "    )\n",
    "\n",
    "    radiant_items, dire_items = [\n",
    "        [f\"{side}{i}_items\" for i in range(1, 6)] for side in [\"r\", \"d\"]\n",
    "    ]\n",
    "\n",
    "    for dataframe in [train_df, test_df]:\n",
    "        for col in dataframe.columns:\n",
    "            dataframe[col] = dataframe[col].apply(ast.literal_eval)\n",
    "\n",
    "    mirror_train_df = train_df[dire_items + radiant_items]\n",
    "    mirror_train_df.columns = train_df.columns\n",
    "    mirror_train_df.index = [\"mirror_\" + idx[7:] for idx in mirror_train_df.index]\n",
    "    train_df = train_df.append(mirror_train_df)\n",
    "\n",
    "    mirror_train_y = ~y_train.astype(bool) * 1\n",
    "    y_train = y_train.append(mirror_train_y)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:07:08.609879Z",
     "start_time": "2020-12-16T22:07:08.599198Z"
    }
   },
   "outputs": [],
   "source": [
    "def item_transform():\n",
    "    train_df, test_df = load_items()\n",
    "    radiant_items, dire_items = [\n",
    "        [f\"{side}{i}_items\" for i in range(1, 6)] for side in [\"r\", \"d\"]\n",
    "    ]\n",
    "\n",
    "    ltostr = lambda row: \" \".join([\" \".join(i) for i in row])\n",
    "    r_temp, d_temp, r_temp_test, d_temp_test = [\n",
    "        df_[side_items].apply(ltostr, axis=1)\n",
    "        for df_ in [train_df, test_df]\n",
    "        for side_items in [radiant_items, dire_items]\n",
    "    ]\n",
    "\n",
    "    rx = r\"{0}[0-9]\".format(\"river_painter\")\n",
    "    r_items, d_items, r_items_test, d_items_test = [\n",
    "        [re.sub(rx, \"river_painter\", x.replace(\"recipe_\", \"\")) for x in side]\n",
    "        for side in [r_temp, d_temp, r_temp_test, d_temp_test]\n",
    "    ]\n",
    "\n",
    "    vectorizer = CountVectorizer().fit(r_items)\n",
    "    r, d, r_test, d_test = [\n",
    "        vectorizer.transform(side_items).toarray()\n",
    "        for side_items in [r_items, d_items, r_items_test, d_items_test]\n",
    "    ]\n",
    "\n",
    "    count_vect_df = pd.DataFrame(r - d, columns=vectorizer.get_feature_names())\n",
    "    count_vect_df_test = pd.DataFrame(\n",
    "        r_test - d_test, columns=vectorizer.get_feature_names()\n",
    "    )\n",
    "\n",
    "    consumables = [\n",
    "        \"tango\",\n",
    "        \"tpscroll\",\n",
    "        \"bottle\",\n",
    "        \"flask\",\n",
    "        \"enchanted_mango\",\n",
    "        \"courier\",\n",
    "        \"clarity\",\n",
    "        \"faerie_fire\",\n",
    "        \"ward_observer\",\n",
    "        \"ward_sentry\",\n",
    "        \"river_painter\",\n",
    "    ]\n",
    "    count_vect_df.drop(columns=consumables, inplace=True)\n",
    "    count_vect_df_test.drop(columns=consumables, inplace=True)\n",
    "\n",
    "    count_vect_df.index = train_df.index\n",
    "    count_vect_df_test.index = test_df.index\n",
    "    return count_vect_df.append(count_vect_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:07:35.397199Z",
     "start_time": "2020-12-16T22:07:09.253136Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.join(item_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:07:38.188992Z",
     "start_time": "2020-12-16T22:07:35.400018Z"
    }
   },
   "outputs": [],
   "source": [
    "X_heroes_train = pd.read_csv(\n",
    "    os.path.join(PATH_TO_ADD_DATA, \"bag_of_heroes_and_win_prob_train.csv\")\n",
    ")\n",
    "X_heroes_test = pd.read_csv(\n",
    "    os.path.join(PATH_TO_ADD_DATA, \"bag_of_heroes_and_win_prob_test.csv\")\n",
    ")\n",
    "\n",
    "X_heroes_train.index = df_train.index\n",
    "X_heroes_test.index = df_test.index\n",
    "\n",
    "X_heroes_train_flip = X_heroes_train.copy()\n",
    "\n",
    "\n",
    "def train_boh_flip(X_heroes_train_flip, X_heroes_train):\n",
    "    players = [f\"f{i}\" for i in range(0, 115)]\n",
    "    for player in players:\n",
    "        X_heroes_train_flip[player] = X_heroes_train[player].map({1: -1, -1: 1, 0: 0})\n",
    "    return X_heroes_train_flip\n",
    "\n",
    "\n",
    "X_heroes_train_flip = train_boh_flip(X_heroes_train_flip, X_heroes_train)\n",
    "\n",
    "\n",
    "def rd_hero_win(X_heroes_train_flip, X_heroes_train):\n",
    "    X_heroes_train_flip[\"r_win_prob\"] = X_heroes_train[\"d_win_prob\"]\n",
    "    X_heroes_train_flip[\"d_win_prob\"] = X_heroes_train[\"r_win_prob\"]\n",
    "    X_heroes_train_flip[\"win_prob\"] = (\n",
    "        X_heroes_train_flip[\"r_win_prob\"] - X_heroes_train_flip[\"d_win_prob\"]\n",
    "    )\n",
    "    return X_heroes_train_flip\n",
    "\n",
    "\n",
    "X_heroes_train_flip = rd_hero_win(X_heroes_train_flip, X_heroes_train)\n",
    "X_heroes_train_flip.index = [\"mirror_\" + idx[7:] for idx in X_heroes_train.index]\n",
    "X_heroes_train_flip.to_csv(\n",
    "    os.path.join(PATH_TO_ADD_DATA, \"bag_of_heroes_and_win_prob_train_flip.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "df = df.join(X_heroes_train.append([X_heroes_train_flip, X_heroes_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:32:58.390539Z",
     "start_time": "2020-12-16T22:31:35.796887Z"
    }
   },
   "outputs": [],
   "source": [
    "df = intel_delete_column_rd(\n",
    "    df, [\"FEP_hero_name\", \"FEM_target_next_roshan_team\", \"FEP_hero_id\"], output=True\n",
    ")\n",
    "df.to_csv(os.path.join(PATH_TO_DATA, \"df_final.csv\"), index=\"match_id_hash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:02:58.891416Z",
     "start_time": "2020-12-16T22:02:38.919994Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = (\n",
    "    pd.read_csv(\n",
    "        os.path.join(PATH_TO_DATA, \"df_final.csv\"), dtype={\"target_radiant_win\": object}\n",
    "    )\n",
    "    .rename(columns={\"Unnamed: 0\": \"match_id_hash\"})\n",
    "    .set_index(\"match_id_hash\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:53.882438Z",
     "start_time": "2020-12-15T11:40:51.745125Z"
    }
   },
   "outputs": [],
   "source": [
    "target_cols = [col for col in df.columns if col.startswith(\"target\")]\n",
    "\n",
    "train = df[~df[\"target_radiant_win\"].isnull()].drop(target_cols, axis=1)\n",
    "test = df[df[\"target_radiant_win\"].isnull()].drop(target_cols, axis=1)\n",
    "\n",
    "train_targets = (\n",
    "    df[~df[\"target_radiant_win\"].isnull()][\"target_radiant_win\"]\n",
    "    .replace({\"True\": 1, \"False\": 0})\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "\n",
    "train_def = train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:56.548320Z",
     "start_time": "2020-12-15T11:40:53.885782Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "\n",
    "\n",
    "# matplotlit and seaborn for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# modeling\n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-16T22:11:28.613773Z",
     "start_time": "2020-12-16T22:11:28.607534Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def to_generalize_features(features, threshold=-1):\n",
    "    fs = []\n",
    "    fc = []\n",
    "    for f in features:\n",
    "        if f[1].isnumeric() and f[0] != \"f\":\n",
    "            fs.append(\"FEP_\" + f[3:])\n",
    "        elif f[1] == \"_\":\n",
    "            fc.append(\"FET_\" + f[2:])\n",
    "        else:\n",
    "            fc.append(\"FEM_\" + f)\n",
    "\n",
    "    sfs = pd.Series(fs)\n",
    "    sfs.value_counts()[sfs.value_counts() > threshold].index\n",
    "\n",
    "    fsfc = list(sfs.value_counts()[sfs.value_counts() > threshold].index) + fc\n",
    "    return fsfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove Collinear Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:56:53.049751Z",
     "start_time": "2020-12-14T13:52:32.356507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = train[:200].corr().abs()\n",
    "corr_matrix.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:56:53.122531Z",
     "start_time": "2020-12-14T13:56:53.052617Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T13:56:53.680765Z",
     "start_time": "2020-12-14T13:56:53.126166Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print(\"There are %d columns to remove.\" % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:56.573339Z",
     "start_time": "2020-12-15T11:40:56.557978Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to_drop\n",
    "to_drop = [\n",
    "    \"d5_hero_melee\",\n",
    "    \"d4_hero_melee\",\n",
    "    \"d3_hero_melee\",\n",
    "    \"d2_hero_melee\",\n",
    "    \"d1_hero_melee\",\n",
    "    \"r5_hero_melee\",\n",
    "    \"r4_hero_melee\",\n",
    "    \"r3_hero_melee\",\n",
    "    \"r2_hero_melee\",\n",
    "    \"r1_hero_melee\",\n",
    "    \"is_ranked_lobby\",\n",
    "    \"r_pt_xp_delta\",\n",
    "    \"r2_xp_delta\",\n",
    "    \"r3_xp_delta\",\n",
    "    \"r4_xp_delta\",\n",
    "    \"r5_xp_delta\",\n",
    "    \"r_pt_damage\",\n",
    "    \"d_pt_xp_delta\",\n",
    "    \"d2_xp_delta\",\n",
    "    \"d3_xp_delta\",\n",
    "    \"d4_xp_delta\",\n",
    "    \"d5_xp_delta\",\n",
    "    \"d_pt_damage\",\n",
    "    \"r1_xp\",\n",
    "    \"r1_nearby_creep_death_count\",\n",
    "    \"r1_camps_stacked\",\n",
    "    \"r1_gold_reasons\",\n",
    "    \"r1_xp_reasons\",\n",
    "    \"r2_xp\",\n",
    "    \"r2_nearby_creep_death_count\",\n",
    "    \"r2_camps_stacked\",\n",
    "    \"r2_gold_reasons\",\n",
    "    \"r2_xp_reasons\",\n",
    "    \"r3_xp\",\n",
    "    \"r3_nearby_creep_death_count\",\n",
    "    \"r3_camps_stacked\",\n",
    "    \"r3_gold_reasons\",\n",
    "    \"r3_xp_reasons\",\n",
    "    \"r4_xp\",\n",
    "    \"r4_nearby_creep_death_count\",\n",
    "    \"r4_camps_stacked\",\n",
    "    \"r4_gold_reasons\",\n",
    "    \"r4_xp_reasons\",\n",
    "    \"r5_xp\",\n",
    "    \"r5_nearby_creep_death_count\",\n",
    "    \"r5_camps_stacked\",\n",
    "    \"r5_gold_reasons\",\n",
    "    \"r5_xp_reasons\",\n",
    "    \"d1_xp\",\n",
    "    \"d1_nearby_creep_death_count\",\n",
    "    \"d1_camps_stacked\",\n",
    "    \"d1_gold_reasons\",\n",
    "    \"d1_xp_reasons\",\n",
    "    \"d2_xp\",\n",
    "    \"d2_nearby_creep_death_count\",\n",
    "    \"d2_camps_stacked\",\n",
    "    \"d2_gold_reasons\",\n",
    "    \"d2_xp_reasons\",\n",
    "    \"d3_xp\",\n",
    "    \"d3_nearby_creep_death_count\",\n",
    "    \"d3_camps_stacked\",\n",
    "    \"d3_gold_reasons\",\n",
    "    \"d3_xp_reasons\",\n",
    "    \"d4_xp\",\n",
    "    \"d4_nearby_creep_death_count\",\n",
    "    \"d4_camps_stacked\",\n",
    "    \"d4_gold_reasons\",\n",
    "    \"d4_xp_reasons\",\n",
    "    \"d5_xp\",\n",
    "    \"d5_nearby_creep_death_count\",\n",
    "    \"d5_camps_stacked\",\n",
    "    \"d5_gold_reasons\",\n",
    "    \"d5_xp_reasons\",\n",
    "    \"r1_xp_pm\",\n",
    "    \"r2_gold_pm\",\n",
    "    \"r2_xp_pm\",\n",
    "    \"r3_gold_pm\",\n",
    "    \"r3_xp_pm\",\n",
    "    \"r4_gold_pm\",\n",
    "    \"r4_xp_pm\",\n",
    "    \"r5_gold_pm\",\n",
    "    \"r5_xp_pm\",\n",
    "    \"d1_xp_pm\",\n",
    "    \"d2_gold_pm\",\n",
    "    \"d2_xp_pm\",\n",
    "    \"d3_gold_pm\",\n",
    "    \"d3_xp_pm\",\n",
    "    \"d4_gold_pm\",\n",
    "    \"d4_xp_pm\",\n",
    "    \"d5_gold_pm\",\n",
    "    \"d5_xp_pm\",\n",
    "    \"r_mean_hero_agi\",\n",
    "    \"d_mean_hero_agi\",\n",
    "    \"r_mean_hero_disabler\",\n",
    "    \"d_mean_hero_disabler\",\n",
    "    \"r_mean_firstblood_claimed\",\n",
    "    \"d_mean_firstblood_claimed\",\n",
    "    \"r_max_firstblood_claimed\",\n",
    "    \"d_max_firstblood_claimed\",\n",
    "    \"r_mean_healing\",\n",
    "    \"d_mean_healing\",\n",
    "    \"r_max_healing\",\n",
    "    \"d_max_healing\",\n",
    "    \"r_mean_xy\",\n",
    "    \"d_mean_xy\",\n",
    "    \"r_mean_hero_nuker\",\n",
    "    \"d_mean_hero_nuker\",\n",
    "    \"r_pt_gold\",\n",
    "    \"d_pt_gold\",\n",
    "    \"r_mean_gold\",\n",
    "    \"d_mean_gold\",\n",
    "    \"r_max_gold\",\n",
    "    \"d_max_gold\",\n",
    "    \"r_min_gold\",\n",
    "    \"d_min_gold\",\n",
    "    \"r_pt_lh\",\n",
    "    \"d_pt_lh\",\n",
    "    \"r_mean_lh\",\n",
    "    \"d_mean_lh\",\n",
    "    \"r_max_lh\",\n",
    "    \"d_max_lh\",\n",
    "    \"r_mean_hero_jungler\",\n",
    "    \"d_mean_hero_jungler\",\n",
    "    \"r_max_hero_jungler\",\n",
    "    \"d_max_hero_jungler\",\n",
    "    \"r_mean_hero_str\",\n",
    "    \"d_mean_hero_str\",\n",
    "    \"r_mean_hero_carry\",\n",
    "    \"d_mean_hero_carry\",\n",
    "    \"r_mean_rune_pickups\",\n",
    "    \"d_mean_rune_pickups\",\n",
    "    \"r_max_rune_pickups\",\n",
    "    \"d_max_rune_pickups\",\n",
    "    \"r_mean_hero_ranged\",\n",
    "    \"d_mean_hero_ranged\",\n",
    "    \"r_mean_roshans_killed\",\n",
    "    \"d_mean_roshans_killed\",\n",
    "    \"r_max_roshans_killed\",\n",
    "    \"d_max_roshans_killed\",\n",
    "    \"r_mean_hero_escape\",\n",
    "    \"d_mean_hero_escape\",\n",
    "    \"r_mean_kda\",\n",
    "    \"d_mean_kda\",\n",
    "    \"r_max_kda\",\n",
    "    \"d_max_kda\",\n",
    "    \"r_mean_obs_placed\",\n",
    "    \"d_mean_obs_placed\",\n",
    "    \"r_max_obs_placed\",\n",
    "    \"d_max_obs_placed\",\n",
    "    \"r_pt_xp\",\n",
    "    \"d_pt_xp\",\n",
    "    \"r_mean_xp\",\n",
    "    \"d_mean_xp\",\n",
    "    \"r_max_xp\",\n",
    "    \"d_max_xp\",\n",
    "    \"r_min_xp\",\n",
    "    \"d_min_xp\",\n",
    "    \"r_mean_hero_pusher\",\n",
    "    \"d_mean_hero_pusher\",\n",
    "    \"r_mean_damage\",\n",
    "    \"d_mean_damage\",\n",
    "    \"r_max_damage\",\n",
    "    \"d_max_damage\",\n",
    "    \"r_mean_towers_killed\",\n",
    "    \"d_mean_towers_killed\",\n",
    "    \"r_max_towers_killed\",\n",
    "    \"d_max_towers_killed\",\n",
    "    \"r_mean_is_mf_hash\",\n",
    "    \"d_mean_is_mf_hash\",\n",
    "    \"r_mean_denies\",\n",
    "    \"d_mean_denies\",\n",
    "    \"r_max_denies\",\n",
    "    \"d_max_denies\",\n",
    "    \"r_mean_hero_initiator\",\n",
    "    \"d_mean_hero_initiator\",\n",
    "    \"r_mean_assists\",\n",
    "    \"d_mean_assists\",\n",
    "    \"r_max_assists\",\n",
    "    \"d_max_assists\",\n",
    "    \"r_min_assists\",\n",
    "    \"d_min_assists\",\n",
    "    \"r_mean_hero_support\",\n",
    "    \"d_mean_hero_support\",\n",
    "    \"r_pt_hero_melee\",\n",
    "    \"d_pt_hero_melee\",\n",
    "    \"r_mean_hero_melee\",\n",
    "    \"d_mean_hero_melee\",\n",
    "    \"r_max_hero_melee\",\n",
    "    \"d_max_hero_melee\",\n",
    "    \"r_min_hero_melee\",\n",
    "    \"d_min_hero_melee\",\n",
    "    \"r_pt_kills\",\n",
    "    \"d_pt_kills\",\n",
    "    \"r_mean_kills\",\n",
    "    \"d_mean_kills\",\n",
    "    \"r_max_kills\",\n",
    "    \"d_max_kills\",\n",
    "    \"r_mean_hero_durable\",\n",
    "    \"d_mean_hero_durable\",\n",
    "    \"r_pt_deaths\",\n",
    "    \"d_pt_deaths\",\n",
    "    \"r_mean_deaths\",\n",
    "    \"d_mean_deaths\",\n",
    "    \"r_max_deaths\",\n",
    "    \"d_max_deaths\",\n",
    "    \"r_mean_item_aegis\",\n",
    "    \"d_mean_item_aegis\",\n",
    "    \"r_max_item_aegis\",\n",
    "    \"d_max_item_aegis\",\n",
    "    \"r_mean_hero_hits\",\n",
    "    \"d_mean_hero_hits\",\n",
    "    \"r_max_hero_hits\",\n",
    "    \"d_max_hero_hits\",\n",
    "    \"r_mean_multi_kills\",\n",
    "    \"d_mean_multi_kills\",\n",
    "    \"r_max_multi_kills\",\n",
    "    \"d_max_multi_kills\",\n",
    "    \"r_mean_hero_int\",\n",
    "    \"d_mean_hero_int\",\n",
    "    \"r_mean_sen_placed\",\n",
    "    \"d_mean_sen_placed\",\n",
    "    \"r_max_sen_placed\",\n",
    "    \"d_max_sen_placed\",\n",
    "    \"r_mean_pred_vict\",\n",
    "    \"d_mean_pred_vict\",\n",
    "    \"r_mean_camps_stacked\",\n",
    "    \"d_mean_camps_stacked\",\n",
    "    \"r_max_camps_stacked\",\n",
    "    \"d_max_camps_stacked\",\n",
    "    \"r_pt_creeps_stacked\",\n",
    "    \"d_pt_creeps_stacked\",\n",
    "    \"r_mean_creeps_stacked\",\n",
    "    \"d_mean_creeps_stacked\",\n",
    "    \"r_max_creeps_stacked\",\n",
    "    \"d_max_creeps_stacked\",\n",
    "    \"r_min_creeps_stacked\",\n",
    "    \"d_min_creeps_stacked\",\n",
    "    \"r_pt_max_health\",\n",
    "    \"d_pt_max_health\",\n",
    "    \"r_mean_max_health\",\n",
    "    \"d_mean_max_health\",\n",
    "    \"r_max_max_health\",\n",
    "    \"d_max_max_health\",\n",
    "    \"r_min_max_health\",\n",
    "    \"d_min_max_health\",\n",
    "    \"r_pt_max_mana\",\n",
    "    \"d_pt_max_mana\",\n",
    "    \"r_mean_max_mana\",\n",
    "    \"d_mean_max_mana\",\n",
    "    \"r_max_max_mana\",\n",
    "    \"d_max_max_mana\",\n",
    "    \"r_mean_stuns\",\n",
    "    \"d_mean_stuns\",\n",
    "    \"r_max_stuns\",\n",
    "    \"d_max_stuns\",\n",
    "    \"r_mean_teamfight_participation\",\n",
    "    \"d_mean_teamfight_participation\",\n",
    "    \"r_mean_gold_t\",\n",
    "    \"d_mean_gold_t\",\n",
    "    \"r_max_gold_t\",\n",
    "    \"d_max_gold_t\",\n",
    "    \"r_mean_lh_t\",\n",
    "    \"d_mean_lh_t\",\n",
    "    \"r_mean_xp_t\",\n",
    "    \"d_mean_xp_t\",\n",
    "    \"r1_y_rot\",\n",
    "    \"r2_y_rot\",\n",
    "    \"r3_y_rot\",\n",
    "    \"r4_y_rot\",\n",
    "    \"r5_y_rot\",\n",
    "    \"d1_y_rot\",\n",
    "    \"d2_y_rot\",\n",
    "    \"d3_y_rot\",\n",
    "    \"d4_y_rot\",\n",
    "    \"d5_y_rot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T16:08:38.053659Z",
     "start_time": "2020-12-14T16:08:37.547453Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO: maybe better to delete all of this\n",
    "#\n",
    "# bf = bad_feats(to_drop, [])\n",
    "# to_delete_cols = intel_delete_column_rd(train, bf, get_pcols=True)\n",
    "# for col in list(set(to_delete_cols) - set(to_drop)):\n",
    "#     print(col, upper[col].max())\n",
    "#####\n",
    "\n",
    "train = train.drop(columns=to_drop)\n",
    "test = test.drop(columns=to_drop)\n",
    "\n",
    "print(\"Training shape: \", train.shape)\n",
    "print(\"Testing shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T14:40:09.926395Z",
     "start_time": "2020-12-14T14:40:09.922653Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Feature Selection through Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:56.598063Z",
     "start_time": "2020-12-15T11:40:56.577636Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, threshold=0.9, doplot=True):\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values(\"importance\", ascending=False).reset_index()\n",
    "\n",
    "    # Normalize the feature importances to add up to one\n",
    "    df[\"importance_normalized\"] = df[\"importance\"] / df[\"importance\"].sum()\n",
    "    df[\"cumulative_importance\"] = np.cumsum(df[\"importance_normalized\"])\n",
    "\n",
    "    if doplot:\n",
    "        # Make a horizontal bar chart of feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.subplot()\n",
    "\n",
    "        # Need to reverse the index to plot most important on top\n",
    "        ax.barh(\n",
    "            list(reversed(list(df.index[:15]))),\n",
    "            df[\"importance_normalized\"].head(15),\n",
    "            align=\"center\",\n",
    "            edgecolor=\"k\",\n",
    "        )\n",
    "\n",
    "        # Set the yticks and labels\n",
    "        ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "        ax.set_yticklabels(df[\"feature\"].head(15))\n",
    "\n",
    "        # Plot labeling\n",
    "        plt.xlabel(\"Normalized Importance\")\n",
    "        plt.title(\"Feature Importances\")\n",
    "        plt.show()\n",
    "\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(list(range(len(df))), df[\"cumulative_importance\"], \"r-\")\n",
    "        plt.xlabel(\"Number of Features\")\n",
    "        plt.ylabel(\"Cumulative Importance\")\n",
    "        plt.title(\"Cumulative Feature Importance\")\n",
    "        plt.show()\n",
    "\n",
    "        importance_index = np.min(np.where(df[\"cumulative_importance\"] > threshold))\n",
    "        print(\n",
    "            \"%d features required for %0.2f of cumulative importance\"\n",
    "            % (importance_index + 1, threshold)\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:56.614210Z",
     "start_time": "2020-12-15T11:40:56.604119Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def identify_zero_importance_features(train, train_labels, iterations=2):\n",
    "    # Initialize an empty array to hold feature importances\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "\n",
    "    # Create the model with several hyperparameters\n",
    "    model = lgb.LGBMClassifier(\n",
    "        objective=\"binary\", boosting_type=\"goss\", n_estimators=10000\n",
    "    )\n",
    "\n",
    "    # Fit the model multiple times to avoid overfitting\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Split into training and validation set\n",
    "        train_features, valid_features, train_y, valid_y = train_test_split(\n",
    "            train, train_labels, test_size=0.25, random_state=SEED\n",
    "        )\n",
    "\n",
    "        # Train using early stopping\n",
    "        model.fit(\n",
    "            train_features,\n",
    "            train_y,\n",
    "            early_stopping_rounds=100,\n",
    "            eval_set=[(valid_features, valid_y)],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=200,\n",
    "        )\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importances += model.feature_importances_ / iterations\n",
    "\n",
    "    feature_importances = pd.DataFrame(\n",
    "        {\"feature\": list(train.columns), \"importance\": feature_importances}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    # Find the features with zero importance\n",
    "    zero_features = list(\n",
    "        feature_importances[feature_importances[\"importance\"] == 0.0][\"feature\"]\n",
    "    )\n",
    "    print(\"\\nThere are %d features with 0.0 importance\" % len(zero_features))\n",
    "\n",
    "    return zero_features, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T11:40:56.626214Z",
     "start_time": "2020-12-15T11:40:56.619777Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_cols_under_treshold(\n",
    "    df,\n",
    "    threshold,\n",
    "    targets,\n",
    "    get_over_threshold=False,\n",
    "    iterations=5,\n",
    "    feature_importances_=None,\n",
    "):\n",
    "    if feats_importances_ is None:\n",
    "        _, feature_importances_ = identify_zero_importance_features(\n",
    "            df, targets, iterations\n",
    "        )\n",
    "\n",
    "    norm_feature_importances = plot_feature_importances(\n",
    "        feature_importances_, threshold=threshold, doplot=False\n",
    "    )\n",
    "    cols_under_cum_imp_threshold = norm_feature_importances[\n",
    "        norm_feature_importances[\"cumulative_importance\"] < threshold\n",
    "    ][\"feature\"].values\n",
    "    if get_over_threshold:\n",
    "        return (\n",
    "            df[cols_under_cum_imp_threshold],\n",
    "            df[set(df.columns) - set(cols_under_cum_imp_threshold)],\n",
    "        )\n",
    "    return df[cols_under_cum_imp_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# FINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T12:55:20.009677Z",
     "start_time": "2020-12-15T12:55:20.001549Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "permutation = np.random.permutation(train_def.shape[0])\n",
    "TEST_STATUS = True\n",
    "\n",
    "if TEST_STATUS:\n",
    "    N_ESTIMATORS = 100\n",
    "    N_CV = 2\n",
    "    RESTRICTION = 200\n",
    "    N_ITERATIONS = 2\n",
    "else:\n",
    "    N_ESTIMATORS = 10000\n",
    "    N_CV = 5\n",
    "    RESTRICTION = 10 ** 5\n",
    "    N_ITERATIONS = 5\n",
    "\n",
    "summaries = []\n",
    "feats_importances_ = None\n",
    "lgb_classifier = lgb.LGBMClassifier(\n",
    "    objective=\"binary\", boosting_type=\"goss\", n_estimators=N_ESTIMATORS\n",
    ")\n",
    "for drop_ in [True, False]:\n",
    "\n",
    "    if drop_:\n",
    "        dropped_train = train_def.drop(columns=to_drop).iloc[permutation][:RESTRICTION]\n",
    "    else:\n",
    "        dropped_train = train_def.copy().iloc[permutation][:RESTRICTION]\n",
    "\n",
    "    if feats_importances_ is None:\n",
    "        _, feats_importances_ = identify_zero_importance_features(\n",
    "            train[:RESTRICTION],\n",
    "            train_targets[permutation][:RESTRICTION],\n",
    "            iterations=N_ITERATIONS,\n",
    "        )\n",
    "\n",
    "    for threshold in [0.95, 0.97, 0.98, 0.99, 999]:\n",
    "        summary = {\"threshold\": threshold, \"drop\": drop_}\n",
    "\n",
    "        train = dropped_train.copy()\n",
    "        train = get_cols_under_treshold(\n",
    "            train[:RESTRICTION],\n",
    "            threshold,\n",
    "            train_targets[permutation][:RESTRICTION],\n",
    "            iterations=N_ITERATIONS,\n",
    "            feature_importances_=feats_importances_,\n",
    "        )\n",
    "\n",
    "        summary[\"cv_score\"] = cross_validate(\n",
    "            estimator=lgb_classifier,\n",
    "            X=train[:RESTRICTION],\n",
    "            y=train_targets[permutation][:RESTRICTION],\n",
    "            cv=N_CV,\n",
    "            scoring=\"roc_auc\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        print(summary[\"threshold\"], summary[\"drop\"])\n",
    "        summaries.append(summary)\n",
    "        print(summary)\n",
    "\n",
    "    feats_importances_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T12:40:36.952664Z",
     "start_time": "2020-12-15T12:40:36.935530Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summaries = [\n",
    "    {\n",
    "        \"threshold\": 0.95,\n",
    "        \"drop\": False,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1551.17084408,\n",
    "                    1549.20896697,\n",
    "                    1553.70101213,\n",
    "                    1547.86408401,\n",
    "                    1551.00747991,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [87.98920989, 90.25121522, 87.72942209, 88.29967999, 89.2156179]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82418862, 0.82313957, 0.83282496, 0.82868008, 0.82804857]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.97,\n",
    "        \"drop\": False,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1801.17132711,\n",
    "                    1801.79713631,\n",
    "                    1807.40070891,\n",
    "                    1802.5189991,\n",
    "                    1802.36746407,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [95.15213203, 94.58149195, 94.04338193, 94.53755307, 94.73621297]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82593396, 0.82426528, 0.83298792, 0.82841072, 0.82996017]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.98,\n",
    "        \"drop\": False,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1983.7437439,\n",
    "                    1983.00015306,\n",
    "                    1982.55795097,\n",
    "                    1987.76440716,\n",
    "                    1981.96202707,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [101.19300508, 100.53555989, 99.56558895, 98.53505683, 99.54745865]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82565301, 0.82328516, 0.83246367, 0.83179779, 0.8302153]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 999,\n",
    "        \"drop\": False,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    2613.45792913,\n",
    "                    2624.33366704,\n",
    "                    2615.64451814,\n",
    "                    2615.51148796,\n",
    "                    2616.90589714,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [93.09312987, 90.40313911, 94.90813398, 93.90510297, 93.01988196]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82622764, 0.8279041, 0.83426052, 0.82976092, 0.82882073]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.98,\n",
    "        \"drop\": True,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    2098.179322,\n",
    "                    2095.39246678,\n",
    "                    2059.01631832,\n",
    "                    2039.11520362,\n",
    "                    1362.83123827,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [31.95075321, 31.42989707, 25.74079227, 25.20989347, 18.99360013]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82021793, 0.82154949, 0.83004849, 0.82684412, 0.82608877]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 999,\n",
    "        \"drop\": True,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1776.4761889,\n",
    "                    1776.09254622,\n",
    "                    1780.76119709,\n",
    "                    1772.02593589,\n",
    "                    1777.50545907,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [94.13745904, 94.74571395, 91.6106019, 92.95641589, 93.26678681]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.82331943, 0.82002151, 0.82986343, 0.82779059, 0.82518124]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.99,\n",
    "        \"drop\": True,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    2135.04629016,\n",
    "                    2137.68749356,\n",
    "                    2106.97155023,\n",
    "                    2139.73715734,\n",
    "                    1419.81394053,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [33.91747642, 31.37089729, 24.38686776, 23.97090578, 18.97910929]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.81959551, 0.81977512, 0.82911504, 0.82423267, 0.82619395]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.95,\n",
    "        \"drop\": True,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1284.54605222,\n",
    "                    1285.85682988,\n",
    "                    1288.0978291,\n",
    "                    1286.75951481,\n",
    "                    1288.83159399,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [89.94498301, 90.11361003, 89.30421901, 89.89981794, 89.86208701]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.81918739, 0.81988874, 0.82735385, 0.82314564, 0.82279818]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.99,\n",
    "        \"drop\": False,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    2894.65649366,\n",
    "                    2894.18757915,\n",
    "                    2896.68515563,\n",
    "                    2891.68123913,\n",
    "                    1958.80399776,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [60.33294344, 59.83159113, 58.85685635, 58.79200721, 26.69429708]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.826693, 0.8251328, 0.83216345, 0.83043508, 0.83017442]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"threshold\": 0.97,\n",
    "        \"drop\": True,\n",
    "        \"cv_score\": {\n",
    "            \"fit_time\": np.array(\n",
    "                [\n",
    "                    1970.23303366,\n",
    "                    1972.56270075,\n",
    "                    1937.96423578,\n",
    "                    1965.52252698,\n",
    "                    1296.1558075,\n",
    "                ]\n",
    "            ),\n",
    "            \"score_time\": np.array(\n",
    "                [30.78945732, 35.06625056, 22.92393661, 23.19641376, 18.31756473]\n",
    "            ),\n",
    "            \"test_score\": np.array(\n",
    "                [0.81853699, 0.82126441, 0.82815407, 0.82634223, 0.82464998]\n",
    "            ),\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_scores = list(map(lambda el: el[\"cv_score\"][\"test_score\"], summaries))\n",
    "df_summaries = pd.DataFrame(\n",
    "    {\n",
    "        \"threshold\": list(map(lambda el: el[\"threshold\"], summaries)),\n",
    "        \"drop\": list(map(lambda el: el[\"drop\"], summaries)),\n",
    "        \"mean\": np.mean(test_scores, axis=1),\n",
    "        \"std\": np.std(test_scores, axis=1),\n",
    "        \"min\": np.min(test_scores, axis=1),\n",
    "        \"max\": np.max(test_scores, axis=1),\n",
    "    }\n",
    ").sort_values(by=[\"mean\", \"std\"], ascending=False)\n",
    "df_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T12:59:29.351771Z",
     "start_time": "2020-12-15T12:59:29.347908Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "COMFORTABLE_FOR_WORK_DF = True\n",
    "\n",
    "if COMFORTABLE_FOR_WORK_DF:\n",
    "    f_drop = True\n",
    "    f_threshold = 0.98\n",
    "else:\n",
    "    f_drop, f_threshold = df_summaries.sort_values(\n",
    "        by=\"mean_score\", ascending=False\n",
    "    ).iloc[0][[\"drop\", \"threshold\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T13:02:02.609631Z",
     "start_time": "2020-12-15T12:59:33.787146Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if f_drop:\n",
    "    train = train_def.drop(columns=to_drop)\n",
    "else:\n",
    "    train = train_def.copy()\n",
    "train = get_cols_under_treshold(\n",
    "    train, f_threshold, train_targets, iterations=N_ITERATIONS\n",
    ")\n",
    "test = test[train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(os.path.join(PATH_TO_DATA, \"final_train.csv\"), index=\"match_id_hash\")\n",
    "test.to_csv(os.path.join(PATH_TO_DATA, \"final_test.csv\"), index=\"match_id_hash\")\n",
    "train_targets.to_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"final_train_targets.csv\"), index=\"match_id_hash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model & GET FINAL PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"final_train.csv\"), index_col=\"match_id_hash\"\n",
    ")\n",
    "test = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"final_test.csv\"), index_col=\"match_id_hash\"\n",
    ")\n",
    "train_targets = pd.read_csv(\n",
    "    os.path.join(PATH_TO_DATA, \"final_train_targets.csv\"), index_col=\"match_id_hash\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "permutation = np.random.permutation(train.shape[0])\n",
    "\n",
    "TEST_RIDE = False\n",
    "if TEST_RIDE:\n",
    "    RESTRICTIONS = 2 * 10 ** 3\n",
    "    N_ITERATIONS = 10\n",
    "    MAX_EVALS = 2\n",
    "    EARLY_STOP = 200\n",
    "else:\n",
    "    RESTRICTIONS = 10 ** 5\n",
    "    N_ITERATIONS = 10 ** 3\n",
    "    MAX_EVALS = 50\n",
    "    EARLY_STOP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train.iloc[permutation][:RESTRICTIONS],\n",
    "    train_targets.iloc[permutation][:RESTRICTIONS][\"target_radiant_win\"].values,\n",
    "    test_size=0.20,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "idxs = []\n",
    "for col in train.columns:\n",
    "    cat = {}\n",
    "    if train[col].nunique() <= 3:\n",
    "        idxs.append(train.columns.get_loc(col))\n",
    "        cat[\"name\"] = col\n",
    "        cat[\"count\"] = train[col].nunique()\n",
    "        cat[\"values\"] = train[col].unique()\n",
    "        cats.append(cat)\n",
    "\n",
    "cats_df = pd.DataFrame(cats, index=idxs).sort_values(by=[\"count\", \"name\"])\n",
    "categorical_features = cats_df[\"name\"].values\n",
    "categorical_features_indices = cats_df[\"name\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y = train.iloc[permutation][:RESTRICTIONS], train_targets.iloc[permutation][:RESTRICTIONS]['target_radiant_win'].values\n",
    "X_validation, y_validation = X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "\n",
    "def hyperopt_objective(params):\n",
    "    model = CatBoostClassifier(\n",
    "        l2_leaf_reg=int(params['l2_leaf_reg']),\n",
    "        learning_rate=params['learning_rate'],\n",
    "        bagging_temperature=params['bagging_temperature'],\n",
    "        iterations=N_ITERATIONS,\n",
    "        eval_metric='AUC',\n",
    "        random_seed=SEED,\n",
    "        verbose=False,\n",
    "        loss_function='Logloss'\n",
    "    )\n",
    "    print(params)\n",
    "    cv_data = cv(\n",
    "        Pool(X, y, cat_features=categorical_features_indices),\n",
    "        model.get_params()\n",
    "    )\n",
    "    best_accuracy = np.max(cv_data['test-AUC-mean'])\n",
    "    \n",
    "    return 1 - best_accuracy # as hyperopt minimises\n",
    "\n",
    "\n",
    "\n",
    "params_space = {\n",
    "    'l2_leaf_reg': hyperopt.hp.qloguniform('l2_leaf_reg', 0, 2, 1),\n",
    "    'learning_rate': hyperopt.hp.uniform('learning_rate', 1e-3, 5e-1),\n",
    "    'bagging_temperature' : hyperopt.hp.uniform('bagging_temperature', 0, 5e-3),\n",
    "    \n",
    "}\n",
    "\n",
    "trials = hyperopt.Trials()\n",
    "\n",
    "best = hyperopt.fmin(\n",
    "    hyperopt_objective,\n",
    "    space=params_space,\n",
    "    algo=hyperopt.tpe.suggest,\n",
    "    max_evals=1, #0 BOL`SHE\n",
    "    trials=trials,\n",
    "    rstate=np.random.RandomState(SEED)\n",
    ")\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cbfinal = CatBoostClassifier(**best,\n",
    "                            iterations=N_ITERATIONS,\n",
    "                            eval_metric='AUC',\n",
    "                            random_seed=SEED,\n",
    "                            verbose=False,\n",
    "                            loss_function='Logloss',\n",
    "                            od_wait=EARLY_STOP,\n",
    "                            od_type='Iter',\n",
    "                           )\n",
    "\n",
    "cv_data = cv(Pool(X, y, cat_features=categorical_features_indices), fmodel.get_params())\n",
    "print('Precise validation accuracy score: {}'.format(np.max(cv_data['test-AUC-mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cbfinal.fit(train, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple lgmbclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T16:36:13.398186Z",
     "start_time": "2020-12-14T16:36:13.394948Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_classifier = lgb.LGBMClassifier(\n",
    "    objective=\"binary\", boosting_type=\"goss\", n_estimators=N_ESTIMATORS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T16:36:51.954934Z",
     "start_time": "2020-12-14T16:36:13.792885Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T16:36:54.099251Z",
     "start_time": "2020-12-14T16:36:51.958272Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "predicted = lgb_classifier.predict(X_train)\n",
    "print(roc_auc_score(predicted, y_train))\n",
    "\n",
    "predicted = lgb_classifier.predict(X_val)\n",
    "print(roc_auc_score(predicted, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate shrinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate * np.power(0.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate * np.power(0.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate * np.power(0.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use test subset for early stopping criterion\n",
    "# This allows us to avoid overtraining and we do not need to optimise the number of trees\n",
    "fit_params = {\n",
    "    \"early_stopping_rounds\": 30,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"eval_set\": [(X_val, y_val)],\n",
    "    \"eval_names\": [\"valid\"],\n",
    "    #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "    \"verbose\": 100,\n",
    "    \"categorical_feature\": \"auto\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use random search, which is more flexible and more efficient than a grid search\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test = {\n",
    "    \"num_leaves\": sp_randint(6, 50),\n",
    "    \"min_child_samples\": sp_randint(100, 500),\n",
    "    \"min_child_weight\": [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "    \"subsample\": sp_uniform(loc=0.2, scale=0.8),\n",
    "    \"colsample_bytree\": sp_uniform(loc=0.4, scale=0.6),\n",
    "    \"reg_alpha\": [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "    \"reg_lambda\": [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "# return_train_score=True\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(\n",
    "    max_depth=-1,\n",
    "    random_state=SEED,\n",
    "    silent=True,\n",
    "    metric=\"None\",\n",
    "    n_jobs=-1,\n",
    "    n_estimators=5000,\n",
    ")\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=param_test,\n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=True,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train, **fit_params)\n",
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_parameters = {\n",
    "#     \"colsample_bytree\": 0.5645098230355559,\n",
    "#     \"min_child_samples\": 250,\n",
    "#     \"min_child_weight\": 100.0,\n",
    "#     \"num_leaves\": 12,\n",
    "#     \"reg_alpha\": 0,\n",
    "#     \"reg_lambda\": 100,\n",
    "#     \"subsample\": 0.8265673984661308,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sw = lgb.LGBMClassifier(**clf.get_params())\n",
    "# set optimal parameters\n",
    "clf_sw.set_params(**opt_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_sample_weight = GridSearchCV(\n",
    "    estimator=clf_sw,\n",
    "    param_grid={\"scale_pos_weight\": [1, 2, 6, 12]},\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    verbose=True,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_sample_weight.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Best score reached: {} with params: {} \".format(\n",
    "        gs_sample_weight.best_score_, gs_sample_weight.best_params_\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valid+-Std     Train  :   Parameters\")\n",
    "for i in np.argsort(gs_sample_weight.cv_results_[\"mean_test_score\"])[-5:]:\n",
    "    print(\n",
    "        \"{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}\".format(\n",
    "            gs_sample_weight.cv_results_[\"params\"][i],\n",
    "            gs_sample_weight.cv_results_[\"mean_test_score\"][i],\n",
    "            gs_sample_weight.cv_results_[\"mean_train_score\"][i],\n",
    "            gs_sample_weight.cv_results_[\"std_test_score\"][i],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_final = lgb.LGBMClassifier(**clf.get_params())\n",
    "# set optimal parameters\n",
    "clf_final.set_params(**opt_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_final.fit(\n",
    "    train,\n",
    "    train_targets,\n",
    "    **fit_params,\n",
    "    callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgmb_predictions = clf_final.predict_proba(test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions.update({\"LGBMClassifier\": lgmb_predictions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO SUBMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T12:32:45.462955Z",
     "start_time": "2020-12-14T12:32:45.422132Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"./data/sample_submission.csv\", index_col=\"match_id_hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T12:32:53.377423Z",
     "start_time": "2020-12-14T12:32:53.373154Z"
    }
   },
   "outputs": [],
   "source": [
    "ss.radiant_win_prob = final_predictions[\"LGBMClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T12:35:59.223189Z",
     "start_time": "2020-12-14T12:35:59.199750Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df_submission = ss.copy()\n",
    "submission_filename = \"submission_{}.csv\".format(\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "df_submission.to_csv(submission_filename)\n",
    "print(\"Submission saved to {}\".format(submission_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "462.517px",
    "left": "1001.99px",
    "right": "20px",
    "top": "388.976px",
    "width": "588.299px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
